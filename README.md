# ğŸ§  SheldonGPT: Building GPT-2 From Scratch (Inspired by The Big Bang Theory)

This project explores how to build a GPT-2-style language model entirely from scratch â€” without relying on Huggingface Trainer or prebuilt utilities. Itâ€™s a step-by-step educational prototype that demystifies the components of modern LLMs.

We use a custom `.txt` file made from **Sheldon Cooperâ€™s iconic dialogue** (manually compiled) as the training dataset.

## ğŸ” Whatâ€™s Inside?

**Part 1 focuses on foundational steps like:**
- Loading and cleaning `.txt` data (`sheldon_dialogue.txt`)
- Tokenization & vocabulary building
- Byte-Pair Encoding (BPE) for subword units
- Special tokens (e.g., `<|endoftext|>`, `<|unk|>`)
- Sliding window sampling of text sequences
- Token embeddings (`[batch, sequence_length, embedding_size]`)

The goal is to walk through every step, from raw text to a trainable input format.

---

## ğŸ“ Files
- `sheldon data v2.ipynb` â€“ full pretraining script from scratch
- `sheldon_dialogue.txt` â€“ Text corpus used for training
- `README.md` â€“ Youâ€™re reading it

## ğŸ¤“ Why Do This?

To understand how transformers work *under the hood* â€” from first principles. No black boxes.

---

## ğŸš€ Coming Soon (Part 2)

In **Part 2**, weâ€™ll dive into:
- Self-attention (from scratch)
- Multi-head attention layers
- Transformer blocks (layer norm, MLPs, GELU)
- Model configs (vocab size, heads, layers, etc.)
- Training loop and decoding strategies (temp, top-k sampling)

---

## ğŸ“– Read the LinkedIn Article  
ğŸ‘‰ [https://www.linkedin.com/posts/emmanuel-ochiba_built-my-own-gpt-2-model-from-scratch-using-activity-7351240765139783680-8_cd?utm_source=share&utm_medium=member_desktop&rcm=ACoAADUBMK0BJKnDNOJm8Uyny8EmQ_c2UBVsWCc]

## â­ï¸ Star this repo if you enjoy the project or learn something new!
